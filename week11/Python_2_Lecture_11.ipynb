{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ch2tjaXGMvaX",
        "LMD9LDWWM1WM",
        "442Be9u8045Z",
        "nFJxlHCp5NkD",
        "_EN4aySlZute",
        "BSd-c9tKayUL",
        "hOiFvsj0a0vj",
        "X_2azJiycAAF",
        "Zf1lW561fJ0B",
        "Fus7p50DhEDX",
        "bm4vHEvihZpQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Продвинутый Python, лекция 11\n",
        "\n",
        "**Лектор:** Петров Тимур\n",
        "\n",
        "**Семинаристы:** Петров Тимур, Коган Александра, Бузаев Федор, Дешеулин Олег\n",
        "\n",
        "**Spoiler Alert:** в рамках курса нельзя изучить ни одну из тем от и до досконально (к сожалению, на это требуется больше времени, чем даже 3 часа в неделю). Но мы попробуем рассказать столько, сколько возможно :)"
      ],
      "metadata": {
        "id": "lnGVAw7xE6rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Продолжаем парсить данные, только поумнее"
      ],
      "metadata": {
        "id": "7xAXzw5rE79y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В прошлый раз мы с вами парсили странички с помощью BS и в целом были этим довольны. Давайте немного усложним задачу: хотим целую систему по парсингу данных!\n",
        "\n",
        "Для этого есть прекрасная бибилотека [Scrapy](https://scrapy.org/)!\n",
        "\n",
        "Суть простая:\n",
        "\n",
        "1. Создаем своего павука, который будет ходить по необходимым сайтам и забирать оттуда результаты\n",
        "\n",
        "2. Результат мы передаем в парсер, чтобы вытащить необходимые нам данные\n",
        "\n",
        "3. Эти данные мы сохраняем в файлики\n",
        "\n",
        "4. ...\n",
        "\n",
        "5. PROFIT"
      ],
      "metadata": {
        "id": "m4IyO3yFFGyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://pustunchik.ua/uploads/school/cache/old/interesting/animals/Chogo-ty-ne-znav-pro-tvaryn/pauki_foto_08.jpg)"
      ],
      "metadata": {
        "id": "LZf0auu9HIbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сегодня мы будем с вами парсить Amazon (доставать оттуда товары и их характеристики)"
      ],
      "metadata": {
        "id": "t6SHyQ9VIpuQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLU5KzmxEz7j"
      },
      "outputs": [],
      "source": [
        "!pip install scrapy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поскольку паук - это уже целый проект, то и соответственно, одни скриптом здесь не обойтись. Поэтому при создании паука через Scrapy мы получаем сразу целую папку"
      ],
      "metadata": {
        "id": "2nNNnR8rHUnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy startproject spider_amazon; cd spider_amazon; ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX6VfAdhHTbp",
        "outputId": "88d26fe1-505a-483e-c057-45e6f9d4343b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'scrapy_amazon', using template directory '/usr/local/lib/python3.7/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/scrapy_amazon\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd scrapy_amazon\n",
            "    scrapy genspider example example.com\n",
            "total 16\n",
            "drwxr-xr-x 3 root root 4096 Oct 29 14:27 .\n",
            "drwxr-xr-x 1 root root 4096 Oct 29 14:27 ..\n",
            "drwxr-xr-x 3 root root 4096 Oct 29 14:27 scrapy_amazon\n",
            "-rw-r--r-- 1 root root  269 Oct 29 14:27 scrapy.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd spider_amazon/spider_amazon; ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuE0kD-GI83z",
        "outputId": "dc597bf8-b386-4200-dc71-16375f725ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 28\n",
            "drwxr-xr-x 3 root root 4096 Oct 29 14:27 .\n",
            "drwxr-xr-x 3 root root 4096 Oct 29 14:27 ..\n",
            "-rw-r--r-- 1 root root    0 Oct 29 14:20 __init__.py\n",
            "-rw-r--r-- 1 root root  268 Oct 29 14:27 items.py\n",
            "-rw-r--r-- 1 root root 3660 Oct 29 14:27 middlewares.py\n",
            "-rw-r--r-- 1 root root  366 Oct 29 14:27 pipelines.py\n",
            "-rw-r--r-- 1 root root 3325 Oct 29 14:27 settings.py\n",
            "drwxr-xr-x 2 root root 4096 Oct 29 14:20 spiders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Что мы здесь видим? Кучу различных питоновских скриптов, которые надо заполнить, чтобы привести нашего паука в действие!\n",
        "\n",
        "Давайте разбираться с каждым отдельно и постигать, как же все это построить. Начнем с наиболее простого"
      ],
      "metadata": {
        "id": "yA_64VKUJM3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Items"
      ],
      "metadata": {
        "id": "Ch2tjaXGMvaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat spider_amazon/spider_amazon/items.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNWSe0mFJcG3",
        "outputId": "ce0285e5-ecba-4254-9cfb-2fed685a6c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Define here the models for your scraped items\n",
            "#\n",
            "# See documentation in:\n",
            "# https://docs.scrapy.org/en/latest/topics/items.html\n",
            "\n",
            "import scrapy\n",
            "\n",
            "\n",
            "class ScrapyAmazonItem(scrapy.Item):\n",
            "    # define the fields for your item here like:\n",
            "    # name = scrapy.Field()\n",
            "    pass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Что такое Item? По существу, это опять-таки dataclass, внутри которого будут храниться необходимые вам поля. Зачем это нужно? Чтобы получать информацию в виде объектов, которые вы можете передать далее и доставать из них нужную информацию\n",
        "\n",
        "Облегчаем себе жизнь, доставая лишь то, что нужно в красивой обертке. Помимо встреоенного scrapy.Item можно также сюда делать непосредственно dataclass и словари. Самое главное - что это то, что вы по итогу получаете\n",
        "\n",
        "Давайте на примере:"
      ],
      "metadata": {
        "id": "18V7xjLrJ0E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "class Product(scrapy.Item):\n",
        "    name = scrapy.Field() # Что такое field? Алиас к словарю (то есть это словарь)\n",
        "    price = scrapy.Field()\n",
        "    stock = scrapy.Field()\n",
        "    tags = scrapy.Field()"
      ],
      "metadata": {
        "id": "SU4m0Jp6K1_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product = Product(name='Desktop PC', price=1000)\n",
        "product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgzaX-vBLLrJ",
        "outputId": "d71c8460-9255-4b17-848c-fb32284fd709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Desktop PC', 'price': 1000}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получили словарь, ничего интересного. Давайте заполним теперь для нашего Амазон-парсера items\n",
        "\n",
        "Какие аттрибуты хотим? Базово скажем:\n",
        "\n",
        "* Название (name)\n",
        "\n",
        "* Цена без скидки (original_price)\n",
        "\n",
        "* Категория (category)\n",
        "\n",
        "* Цена со скидкой (sale_price)\n",
        "\n",
        "* Можно ли купить (availability)"
      ],
      "metadata": {
        "id": "ZZT2tLhjLblC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Заносим в ```items.py```"
      ],
      "metadata": {
        "id": "ZPswCxEOL7Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "class SpiderAmazonItem(scrapy.Item):\n",
        "    # define the fields for your item here like:\n",
        "    product_name = scrapy.Field()\n",
        "    product_sale_price = scrapy.Field()\n",
        "    product_category = scrapy.Field()\n",
        "    product_original_price = scrapy.Field()\n",
        "    product_availability = scrapy.Field()"
      ],
      "metadata": {
        "id": "U0jypQ5xL6Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, настроили коробку для презентации наших результатов, двинемся дальше"
      ],
      "metadata": {
        "id": "HLEPXKuEMi4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spiders"
      ],
      "metadata": {
        "id": "LMD9LDWWM1WM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spiders - это основа нашего проекта, паучок, который будет ходить по сайту и забирать данные, чтобы потом передавать их на обработку. Если сейчас посмотреть внутри папки Spiders, то там есть только init, в котором ничего нет. Давайте создадим дефолтного паука и будем его оснащать"
      ],
      "metadata": {
        "id": "7UGZ-NttM6f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy genspider AmazonProductSpider amazon.com"
      ],
      "metadata": {
        "id": "O-8OJrXeNxbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, создался некоторый дефолтный паук (внутри папки spiders файл AmazonProductSpider.py)\n",
        "\n",
        "```\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class AmazonproductspiderSpider(scrapy.Spider):\n",
        "    name = 'AmazonProductSpider'\n",
        "    allowed_domains = ['amazon.com']\n",
        "    start_urls = ['http://amazon.com/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        pass\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "9S_Tnm2DNz_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Что тут есть изначально?\n",
        "\n",
        "* name - название нашего паука (к которому мы сможем обращаться из командной строки)\n",
        "\n",
        "* allowed_domains - список доменов, внутри которых наш паук может передвигаться (по дефолту ограничиваем, что только по amazon, на остальные сайты не ходим)\n",
        "\n",
        "* start_urls - с каких ссылок мы начинаем ходить\n",
        "\n",
        "После того, как мы зашли на страничку, далее происходит parse, в качестве аргумента которого передается response, то есть контент, который мы получили от GET-запроса. Для того, чтобы написать parse, нам надо обратиться уже к сайту Amazon"
      ],
      "metadata": {
        "id": "FG2DyfF8OM38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возьмем несколько ссылок по флэшкам, например:\n",
        "\n",
        "* https://www.amazon.com/dp/B00JES3MO0\n",
        "\n",
        "* https://www.amazon.com/dp/B08NCC24HV\n",
        "\n",
        "* https://www.amazon.com/dp/B09LCDF2VD\n",
        "\n",
        "* https://www.amazon.com/dp/B0B3D8Z7T8"
      ],
      "metadata": {
        "id": "7urLPIJyRN0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "s = \"\"\n",
        "with open(\"Amazon.txt\", 'r') as f:\n",
        "    s = f.read()\n",
        "soup = BeautifulSoup(s, 'html.parser') # указываем парсер\n",
        "print(soup.prettify()) # выглядит уже более структурно"
      ],
      "metadata": {
        "id": "bjpIIG8zpFqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Справедливости ради, выглядит ужасно, иногда проще просто по самой странице искать, что просто по HTML-коду. Давайте разыскивать все, что нам надо"
      ],
      "metadata": {
        "id": "QPV7TySHqj2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Что видим?\n",
        "\n",
        "* Название - ```id=productTitle```\n",
        "\n",
        "* Категория - ```<a class=\"a-link-normal a-color-tertiary\" ... > </a> ```\n",
        "\n",
        "* Цена без скидки -  ```<span class=\"... a-price a-text-price'\">```\n",
        "\n",
        "* Цена со скидкой - ```<span class=\"... priceToPay\">```\n",
        "\n",
        "* Можно ли купить - ```<div id=\"availability\" ... > ```"
      ],
      "metadata": {
        "id": "LxuOjmnKSYX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all(id='productTitle')[0].text.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VvaI_NGqtnx",
        "outputId": "f2310482-464b-4d62-85d5-72af929ec9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Enfain 16GB USB 2.0 Flash Memory Stick Drive Swivel Thumb Drives Bulk 10 Pack Black, Portable Data Storage for Universal Purposes at Home & The Office'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all(class_ = 'a-link-normal a-color-tertiary')[0].text.strip() + '/' + \\\n",
        "soup.find_all(class_ = 'a-link-normal a-color-tertiary')[1].text.strip() + '/' + \\\n",
        "soup.find_all(class_ = 'a-link-normal a-color-tertiary')[2].text.strip() + '/' + \\\n",
        "soup.find_all(class_ = 'a-link-normal a-color-tertiary')[3].text.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAKOzHwSrThh",
        "outputId": "e55773e9-781e-4ea5-da7a-479110d295b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Electronics/Computers & Accessories/Data Storage/USB Flash Drives'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all(class_ = 'a-price a-text-price')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcO3jZEOzqeN",
        "outputId": "00edca16-01f1-430e-92b7-3613165d41c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<span class=\"a-price a-text-price\" data-a-color=\"secondary\" data-a-size=\"s\" data-a-strike=\"true\"><span class=\"a-offscreen\">$30.80</span><span aria-hidden=\"true\">$30.80</span></span>]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all(class_ = 'a-price a-text-price')[0].span.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M4rLd8Es8b2",
        "outputId": "0eb70f0e-8865-47db-9748-afa29228a4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$30.80'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all(class_ = \"priceToPay\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqiwIhYVzW7g",
        "outputId": "45815f0b-25bc-4329-9adf-47de67e24e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<span class=\"a-price aok-align-center reinventPricePriceToPayMargin priceToPay\" data-a-color=\"base\" data-a-size=\"xl\"><span class=\"a-offscreen\">$24.80</span><span aria-hidden=\"true\"><span class=\"a-price-symbol\">$</span><span class=\"a-price-whole\">24<span class=\"a-price-decimal\">.</span></span><span class=\"a-price-fraction\">80</span></span></span>]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all(class_ = \"priceToPay\")[0].span.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhtpTxZtrqje",
        "outputId": "3eaadc09-3939-44cc-ff4c-8a1377b27063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$24.80'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all(id='availability')[0].span.text.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YADi8sSHrhjb",
        "outputId": "23860623-3c7b-4cd2-aa57-ee4b04dbd780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In Stock.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ура, все нашли, но выглядит не очень, но все равно надо как-то записать, да и еще через scrapy. Но чтобы облегчить нашу работу, погорим про такую вещь, как XPath (XML Path Language)"
      ],
      "metadata": {
        "id": "a0uZeIRwuLbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xpath"
      ],
      "metadata": {
        "id": "442Be9u8045Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы с вами на семинаре говорили, что XML, по своему суещству, это та же HTML-разметка, только с меньшими условностями (можно заводить вообще любые тэги)\n",
        "\n",
        "А это значит, что HTML-документ можно рассматривать как пример XML-документа и его парсить!\n",
        "\n"
      ],
      "metadata": {
        "id": "6RccGvz-08dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import etree\n",
        "\n",
        "dom = etree.HTML(str(soup))"
      ],
      "metadata": {
        "id": "LrdNUVIOxVI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XPath - это язык для путей внутри XML. То есть можно в одну строчку написать, как пройти к необходимому полю, частично мы такое уже видели в BS (через, например, a/span - вложенность span внутри a)\n",
        "\n",
        "Можно считать, что XPath - это определенные регулярки, только для файлов. Давайте разберемся в базовом синтаксисе [XPath](http://www.k-press.ru/cs/2001/2/XPath/XPath4.asp):"
      ],
      "metadata": {
        "id": "uJr7VoP21TQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* /  - спуск по иерархии (прямо как с директориями-файлами)\n",
        "\n",
        "* // - рекурсивный спуск (вытягиваем все внутри на любой глубине)\n",
        "\n",
        "* . - где мы находимся сейчас\n",
        "\n",
        "* @ - атрибут (например, если мы ищем все span с class=\"abc\", то это будет @class=\"abc\"\n",
        "\n",
        "* \\* - выбираем все (зачем нужно: допустим, что мы хотим найти все a внутри span на 1 уровень ниже, то есть span/что угодно/a, тогда запись будет как span/*/a)\n",
        "\n",
        "* [] - фильтр и индексация\n",
        "\n",
        "* and, or, not - логические операции\n",
        "\n",
        "* contains - поиск по тому, присутствует ли нужный атрибут (например, contains(@class, \"abc\") - если есть атрибут class и внутри class есть \"abc\")\n",
        "\n",
        "Разберем на примере:"
      ],
      "metadata": {
        "id": "TdmwKslV1rc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "//span[@class=\"a-price a-text-price\"]/span[@class=\"a-offscreen\"]/text()\n",
        "```\n",
        "\n",
        "1. //span - найди рекурсивно тэг span (вне зависимости от уровня)\n",
        "\n",
        "2. [@class=\"a-price a-text-price\"] - отфильтруй их по признаку class = \"a-price a-text-price\"\n",
        "\n",
        "3. /span[@class=\"a-offscreen\"] - Перейди по отфильтрованным в дочерний span с нужным классом\n",
        "\n",
        "4. /text() - вытяни текст"
      ],
      "metadata": {
        "id": "X6617WFX3YNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dom.xpath('//span[@class=\"a-price a-text-price\"]/span[@class=\"a-offscreen\"]/text()')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDhqXpWY37Ws",
        "outputId": "f13f4ddc-be09-46b9-fa56-d8fdb51a74f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['$30.80']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dom.xpath('//span[@class=\"a-price a-text-price\"]/span[1]//text()') # аналогично, только переходим во второй span"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a4uQ0ws4A_u",
        "outputId": "56ce132d-ede6-419c-e274-b96208d24990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['$30.80']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Второй пример:\n",
        "\n",
        "```\n",
        "//span[contains(@class,\"priceToPay\")]/span[@class=\"a-offscreen\"]//text()\n",
        "```\n",
        "\n",
        "Здесь разница только в том, что мы используем contains (то есть если в классе есть priceToPay, то забираем). Почему здесь используем contains? Потому что если посмотреть выше, то там не только этот класс присутствует"
      ],
      "metadata": {
        "id": "CVRHYy1V4eDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте запишем:"
      ],
      "metadata": {
        "id": "flAnNjJMUi-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(response):\n",
        "    items = SpiderAmazonItem()\n",
        "    title = response.xpath('//span[@id=\"productTitle\"]/text()')#.extract() - внутри scrapy работает тот же xpath, но только результат надо отдельно вытаскивать\n",
        "    sale_price = response.xpath('//span[contains(@class,\"priceToPay\")]/span[@class=\"a-offscreen\"]//text()')#.extract()\n",
        "    full_price = response.xpath('//span[@class=\"a-price a-text-price\"]/span[@class=\"a-offscreen\"]//text()')#.extract()\n",
        "    category = response.xpath('//a[@class=\"a-link-normal a-color-tertiary\"]/text()')#.extract()\n",
        "    availability = response.xpath('//div[@id=\"availability\"]/span//text()')#.extract()\n",
        "    items['product_name'] = ''.join(title).strip()\n",
        "    items['product_sale_price'] = ''.join(sale_price).strip()\n",
        "    items['product_category'] = '/'.join(map(lambda x: x.strip(), category)).strip()\n",
        "    items['product_availability'] = ''.join(availability).strip()\n",
        "    items['product_original_price'] = ''.join(full_price).strip()\n",
        "    yield items"
      ],
      "metadata": {
        "id": "qhhAQdfYuWdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(parse(dom))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXKay6OuuoOe",
        "outputId": "34d8e8af-ff93-48ff-b4a3-144d60bd6daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product_availability': 'In Stock.',\n",
              " 'product_category': 'Electronics/Computers & Accessories/Data Storage/USB '\n",
              "                     'Flash Drives',\n",
              " 'product_name': 'Enfain 16GB USB 2.0 Flash Memory Stick Drive Swivel Thumb '\n",
              "                 'Drives Bulk 10 Pack Black, Portable Data Storage for '\n",
              "                 'Universal Purposes at Home & The Office',\n",
              " 'product_original_price': '$30.80',\n",
              " 'product_sale_price': '$24.80'}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ура, разобрались! Можно двигаться дальше!"
      ],
      "metadata": {
        "id": "7ueINf1r5KmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дособираем паука"
      ],
      "metadata": {
        "id": "nFJxlHCp5NkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "from spider_amazon.items import SpiderAmazonItem\n",
        "\n",
        "class AmazonproductspiderSpider(scrapy.Spider):\n",
        "    name = 'AmazonProductSpider'\n",
        "    allowed_domains = ['amazon.com']\n",
        "    start_urls = ['https://www.amazon.com/dp/B00JES3MO0', 'https://www.amazon.com/dp/B08NCC24HV',\n",
        "        'https://www.amazon.com/dp/B09LCDF2VD', 'https://www.amazon.com/dp/B0B3D8Z7T8']\n",
        "\n",
        "    def parse(self, response):\n",
        "        items = SpiderAmazonItem()\n",
        "        title = response.xpath('//span[@id=\"productTitle\"]/text()').extract()\n",
        "        sale_price = response.xpath('//span[contains(@class,\"priceToPay\")]/span[@class=\"a-offscreen\"]//text()').extract()\n",
        "        full_price = response.xpath('//span[@class=\"a-price a-text-price\"]/span[@class=\"a-offscreen\"]//text()').extract()\n",
        "        category = response.xpath('//a[@class=\"a-link-normal a-color-tertiary\"]/text()').extract()\n",
        "        availability = response.xpath('//div[@id=\"availability\"]/span//text()').extract()\n",
        "        items['product_name'] = ''.join(title).strip()\n",
        "        items['product_sale_price'] = ''.join(sale_price).strip()\n",
        "        items['product_category'] = '/'.join(map(lambda x: x.strip(), category)).strip()\n",
        "        items['product_availability'] = ''.join(availability).strip()\n",
        "        items['product_original_price'] = ''.join(full_price).strip()\n",
        "        yield items"
      ],
      "metadata": {
        "id": "RsKlsIPzUkt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, мы с вами написали код, который что-то будет выдавать! Осталось это все дело связать"
      ],
      "metadata": {
        "id": "WtQZ8iPSZv45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipelines"
      ],
      "metadata": {
        "id": "_EN4aySlZute"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все, что мы получаем после обработки (когда выдаем yield в пауке), мы передаем в pipeline - это обработка полученного результата\n",
        "\n",
        "```\n",
        "# Define your item pipelines here\n",
        "#\n",
        "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
        "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
        "\n",
        "\n",
        "# useful for handling different item types with a single interface\n",
        "from itemadapter import ItemAdapter\n",
        "\n",
        "\n",
        "class SpiderAmazonPipeline:\n",
        "    def process_item(self, item, spider):\n",
        "        return item\n",
        "```"
      ],
      "metadata": {
        "id": "1laEvKKmaIW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вызывается функция process_item, внутри которой можно обработать получившиеся items из паука (иногда мы хотим что-нибудь отфильтровать, добавить в json etc)\n",
        "\n",
        "Давайте добавим функционал, чтобы наши результаты можно было добавлять в json файлик"
      ],
      "metadata": {
        "id": "WhiGsM58aa7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from itemadapter import ItemAdapter #класс для обработки items, можно проверить, получили ли мы item\n",
        "\n",
        "class SpiderAmazonPipeline:\n",
        "\n",
        "    def open_spider(self, spider): # что делать при открытии паука (создаем файлик)\n",
        "        self.file = open('items.json', 'w')\n",
        "\n",
        "    def close_spider(self, spider): # что делать при окончании работы паука (закрываем файлик)\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider): #что делать с полученным item\n",
        "        line = json.dumps(ItemAdapter(item).asdict()) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item"
      ],
      "metadata": {
        "id": "4oY14OIa7Mbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Middlewares"
      ],
      "metadata": {
        "id": "BSd-c9tKayUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Исходя из названия это некоторый \"промежуточный слой\". Что он в себе таит?\n",
        "\n",
        "```\n",
        "# Define here the models for your spider middleware\n",
        "#\n",
        "# See documentation in:\n",
        "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "\n",
        "from scrapy import signals\n",
        "\n",
        "# useful for handling different item types with a single interface\n",
        "from itemadapter import is_item, ItemAdapter\n",
        "\n",
        "\n",
        "class SpiderAmazonSpiderMiddleware:\n",
        "    # Not all methods need to be defined. If a method is not defined,\n",
        "    # scrapy acts as if the spider middleware does not modify the\n",
        "    # passed objects.\n",
        "\n",
        "    @classmethod\n",
        "    def from_crawler(cls, crawler):\n",
        "        # This method is used by Scrapy to create your spiders.\n",
        "        s = cls()\n",
        "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
        "        return s\n",
        "\n",
        "    def process_spider_input(self, response, spider):\n",
        "        # Called for each response that goes through the spider\n",
        "        # middleware and into the spider.\n",
        "\n",
        "        # Should return None or raise an exception.\n",
        "        return None\n",
        "\n",
        "    def process_spider_output(self, response, result, spider):\n",
        "        # Called with the results returned from the Spider, after\n",
        "        # it has processed the response.\n",
        "\n",
        "        # Must return an iterable of Request, or item objects.\n",
        "        for i in result:\n",
        "            yield i\n",
        "\n",
        "    def process_spider_exception(self, response, exception, spider):\n",
        "        # Called when a spider or process_spider_input() method\n",
        "        # (from other spider middleware) raises an exception.\n",
        "\n",
        "        # Should return either None or an iterable of Request or item objects.\n",
        "        pass\n",
        "\n",
        "    def process_start_requests(self, start_requests, spider):\n",
        "        # Called with the start requests of the spider, and works\n",
        "        # similarly to the process_spider_output() method, except\n",
        "        # that it doesn’t have a response associated.\n",
        "\n",
        "        # Must return only requests (not items).\n",
        "        for r in start_requests:\n",
        "            yield r\n",
        "\n",
        "    def spider_opened(self, spider):\n",
        "        spider.logger.info('Spider opened: %s' % spider.name)\n",
        "\n",
        "\n",
        "class SpiderAmazonDownloaderMiddleware:\n",
        "    # Not all methods need to be defined. If a method is not defined,\n",
        "    # scrapy acts as if the downloader middleware does not modify the\n",
        "    # passed objects.\n",
        "\n",
        "    @classmethod\n",
        "    def from_crawler(cls, crawler):\n",
        "        # This method is used by Scrapy to create your spiders.\n",
        "        s = cls()\n",
        "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
        "        return s\n",
        "\n",
        "    def process_request(self, request, spider):\n",
        "        # Called for each request that goes through the downloader\n",
        "        # middleware.\n",
        "\n",
        "        # Must either:\n",
        "        # - return None: continue processing this request\n",
        "        # - or return a Response object\n",
        "        # - or return a Request object\n",
        "        # - or raise IgnoreRequest: process_exception() methods of\n",
        "        #   installed downloader middleware will be called\n",
        "        return None\n",
        "\n",
        "    def process_response(self, request, response, spider):\n",
        "        # Called with the response returned from the downloader.\n",
        "\n",
        "        # Must either;\n",
        "        # - return a Response object\n",
        "        # - return a Request object\n",
        "        # - or raise IgnoreRequest\n",
        "        return response\n",
        "\n",
        "    def process_exception(self, request, exception, spider):\n",
        "        # Called when a download handler or a process_request()\n",
        "        # (from other downloader middleware) raises an exception.\n",
        "\n",
        "        # Must either:\n",
        "        # - return None: continue processing this exception\n",
        "        # - return a Response object: stops process_exception() chain\n",
        "        # - return a Request object: stops process_exception() chain\n",
        "        pass\n",
        "\n",
        "    def spider_opened(self, spider):\n",
        "        spider.logger.info('Spider opened: %s' % spider.name)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "xT_vbyRC8BYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "По существу это любая обработка поведения паука (при вызове, при выдаче результата etc), здесь таится логирование, непосредственно вызов самого паука etc. Здесь ничего менять не будем"
      ],
      "metadata": {
        "id": "ZW124D-t8d49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Settings"
      ],
      "metadata": {
        "id": "hOiFvsj0a0vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь хранятся настройки нашего проекта\n",
        "\n",
        "```\n",
        "# Scrapy settings for spider_amazon project\n",
        "#\n",
        "# For simplicity, this file contains only settings considered important or\n",
        "# commonly used. You can find more settings consulting the documentation:\n",
        "#\n",
        "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
        "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "\n",
        "BOT_NAME = 'spider_amazon'\n",
        "\n",
        "SPIDER_MODULES = ['spider_amazon.spiders']\n",
        "NEWSPIDER_MODULE = 'spider_amazon.spiders'\n",
        "\n",
        "\n",
        "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
        "#USER_AGENT = 'spider_amazon (+http://www.yourdomain.com)'\n",
        "\n",
        "# Obey robots.txt rules\n",
        "ROBOTSTXT_OBEY = True\n",
        "\n",
        "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
        "#CONCURRENT_REQUESTS = 32\n",
        "\n",
        "# Configure a delay for requests for the same website (default: 0)\n",
        "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
        "# See also autothrottle settings and docs\n",
        "#DOWNLOAD_DELAY = 3\n",
        "# The download delay setting will honor only one of:\n",
        "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
        "#CONCURRENT_REQUESTS_PER_IP = 16\n",
        "\n",
        "# Disable cookies (enabled by default)\n",
        "#COOKIES_ENABLED = False\n",
        "\n",
        "# Disable Telnet Console (enabled by default)\n",
        "#TELNETCONSOLE_ENABLED = False\n",
        "\n",
        "# Override the default request headers:\n",
        "#DEFAULT_REQUEST_HEADERS = {\n",
        "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "#   'Accept-Language': 'en',\n",
        "#}\n",
        "\n",
        "# Enable or disable spider middlewares\n",
        "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "#SPIDER_MIDDLEWARES = {\n",
        "#    'spider_amazon.middlewares.SpiderAmazonSpiderMiddleware': 543,\n",
        "#}\n",
        "\n",
        "# Enable or disable downloader middlewares\n",
        "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "#DOWNLOADER_MIDDLEWARES = {\n",
        "#    'spider_amazon.middlewares.SpiderAmazonDownloaderMiddleware': 543,\n",
        "#}\n",
        "\n",
        "# Enable or disable extensions\n",
        "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
        "#EXTENSIONS = {\n",
        "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
        "#}\n",
        "\n",
        "# Configure item pipelines\n",
        "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
        "ITEM_PIPELINES = {\n",
        "   'spider_amazon.pipelines.SpiderAmazonPipeline': 300,\n",
        "}\n",
        "\n",
        "# Enable and configure the AutoThrottle extension (disabled by default)\n",
        "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
        "#AUTOTHROTTLE_ENABLED = True\n",
        "# The initial download delay\n",
        "#AUTOTHROTTLE_START_DELAY = 5\n",
        "# The maximum download delay to be set in case of high latencies\n",
        "#AUTOTHROTTLE_MAX_DELAY = 60\n",
        "# The average number of requests Scrapy should be sending in parallel to\n",
        "# each remote server\n",
        "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
        "# Enable showing throttling stats for every response received:\n",
        "#AUTOTHROTTLE_DEBUG = False\n",
        "\n",
        "# Enable and configure HTTP caching (disabled by default)\n",
        "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
        "#HTTPCACHE_ENABLED = True\n",
        "#HTTPCACHE_EXPIRATION_SECS = 0\n",
        "#HTTPCACHE_DIR = 'httpcache'\n",
        "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
        "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n",
        "\n",
        "# Set settings whose default value is deprecated to a future-proof value\n",
        "REQUEST_FINGERPRINTER_IMPLEMENTATION = '2.7'\n",
        "TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n",
        "```"
      ],
      "metadata": {
        "id": "-3dhH13UbMIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Какие настройки надо включить (включаем ItemPipelines)\n",
        "\n",
        "Что изначально включено?\n",
        "\n",
        "* ROBOTSTXT_OBEY = True - мы не вредоносные товарищи, поэтому следуем правилам для роботов (иначе получим пермабан для нашего IP)\n",
        "\n",
        "Что за число 300 у ItemPipelines? Это приоритет (мы можем создать несколько обработчиков, берем по приоритету, числа от 1 до 1000, чем больше - тем приоритетнее, если один не справился, то берет следующий по приоритету)"
      ],
      "metadata": {
        "id": "m76sP6MMb-bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Запускаем все это добро!"
      ],
      "metadata": {
        "id": "X_2azJiycAAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упс, что-то не сработало. Давайте смотреть в логи:\n",
        "\n",
        "503, 503, 503... Amazon спокойно задетектил, что мы роботы и ничего не дал, плак-плак. Надо такие вещи обходить!\n",
        "\n",
        "Делать будем это с помощью [ScraperAPI](https://www.scraperapi.com/)!"
      ],
      "metadata": {
        "id": "3r8P-HcDcFXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Что это такое? Давайте вначале в целом поговорим, зачем ботов банить\n",
        "\n",
        "В сети ходит миллионы (если не больше) различных ботов, которые собирают ту или иную информацию. При этом есть хорошие боты (типа Facebook crawler, который собирает информацию для того, чтобы делать вам лучше рекомендации), а есть различные вредоносные боты, которые пытаются сломать сайт (DDOS-атаки: давайте нагрузим сервер огромным числом запросов, чтобы он упал), хакнуть вам пользователей, использовать в качестве конкуренции (собирать цены на вашем сайте, а потом на их основе вас немного демпинговать) etc\n",
        "\n",
        "По дефолту сам владелец указывает, что есть хорошо, а что есть плохо. Например, если ему надо собирать статистику etc, то он позволит Facebook Crawler собирать инфу. А всем остальным мы ограничиваем доступ. Если вы посмотрите в логи выполнения нашего crawler, то можете увидеть в первую очередь вот такую вещь:\n",
        "\n",
        "https://www.amazon.com/robots.txt\n",
        "\n",
        "Это документ, который говорит, что можно роботу, а что нельзя роботу (и как можно видеть, почти ничего нельзя)\n"
      ],
      "metadata": {
        "id": "20Ckp37OZMq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "А как можно задетектить бота?\n",
        "\n",
        "* Слишком много запросов - не похоже на человека\n",
        "\n",
        "* Слишком быстро уходишь с сайта - роботы это делают за миллисекунды, люди помедленнее будут\n",
        "\n",
        "* Слишком быстрые сессии - боты быстрее людей\n",
        "\n",
        "* Иногда можно задетектить по самим ссылкам - попробуйте зайти в условный маркет, тыкнуть на товар и посмотреть на ссылку:\n",
        "\n",
        "https://market.yandex.ru/product--veb-kamera-logitech-c922-pro-stream/1711878907?skuId=100328764984&sku=100328764984&show-uid=16671261826754605352306000&offerid=OLIPX0hJS-c6i07BZxtEkA&cpc=A1G1H7OJVLmd9_9T1L4EN-tlgcEeW9fAiA2QpnL7U7bOkvGq2sEud-N9sRb5D2VJ36LmEAtHnxbc7itnzvh72MQSmhOyyMgNZYq35-mCHZnkizKWYr-vWBb62Ra9_WpSweQK7HWvl1Advup0V_KVFdX2nw3-e36pyj-ClcKbOeSZayQTaWxcu4OEcsESPBjY&no-pda-redir=1\n",
        "\n",
        "При этом чтобы попасть на тот же товар, вам достаточно всего лишь:\n",
        "\n",
        "https://market.yandex.ru/product--veb-kamera-logitech-c922-pro-stream/1711878907\n",
        "\n",
        "Все остальное - это различные параметры и тэги, которые потом нужны для маркетинговых исселдований трафика etc. Человек, пришедший без ссылок - это тоже подозрительно\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cp44GDqAc9XI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно очень напрячься и все это учитывать (заходить с разных IP etc), но поскольку web crawling - это достаточно популярная вещь, то есть много сервисов, которые вам позволяют получить прокси, которая самостоятельно будет проходить через капчи и прочие пакости и имитировтаь человеческое поведение (при этом если вы не делаете какую-то гадость)\n",
        "\n",
        "Вот ScraperAPI - одна из таких утилит.\n",
        "\n",
        "Нужно зарегистрироваться и вы сразу получаете бесплатную версию (где можно делать до 5000 запросов в месяц по 5 потокам одновременно, больше запросов уже за денежку)\n",
        "\n",
        "Вы получаете API-ключ, через который вы авторизируетесь, что это ваш запрос, который Scraper прокинет через свои сервера и настройки, в конечном итоге вы получаете доступ!\n",
        "\n"
      ],
      "metadata": {
        "id": "GqW3h20ceT6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Уходим от блокирования роботом"
      ],
      "metadata": {
        "id": "Zf1lW561fJ0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Что нужно сделать? Надо добавить в паучка проход через ScraperAPI!\n",
        "\n",
        "Добавляем к нашему пауку вне функцию (добавляем функцию для сборки нашего urlа через [urllib.requests](https://docs.python.org/3/library/urllib.request.html) - полезная библиотека, чтобы спарсить url или собрать его как нам надо. Как вообще работают url-ссылки поговорим на семинаре"
      ],
      "metadata": {
        "id": "0qum9gb5fNvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from urllib.parse import urlencode\n",
        "from spider_amazon.items import SpiderAmazonItem\n",
        "\n",
        "API = 'YOUR_TOKEN'\n",
        "\n",
        "def get_url(url):\n",
        "    payload = {'api_key': API, 'url': url}\n",
        "    proxy_url = 'http://api.scraperapi.com/?' + urlencode(payload) # обращаемся к ScraperAPI, указывая необходимый сайт и API ключ\n",
        "    return proxy_url\n",
        "\n",
        "class AmazonproductspiderSpider(scrapy.Spider):\n",
        "    name = 'AmazonProductSpider'\n",
        "    allowed_domains = ['amazon.com']\n",
        "    start_urls = ['https://www.amazon.com/dp/B00JES3MO0', 'https://www.amazon.com/dp/B08NCC24HV',\n",
        "                  'https://www.amazon.com/dp/B09LCDF2VD', 'https://www.amazon.com/dp/B0B3D8Z7T8']\n",
        "\n",
        "    def start_requests(self): # придется переписать start_requests - функция, которая работает с GET-запросами\n",
        "        for url in self.start_urls:\n",
        "            yield scrapy.Request(url=get_url(url), callback=self.parse)\n",
        "\n",
        "    # Как эта функция выглядит по дефолту\n",
        "        # def start_requests(self):\n",
        "        # if not self.start_urls and hasattr(self, 'start_url'):\n",
        "        #     raise AttributeError(\n",
        "        #         \"Crawling could not start: 'start_urls' not found \"\n",
        "        #         \"or empty (but found 'start_url' attribute instead, \"\n",
        "        #         \"did you miss an 's'?)\")\n",
        "        # for url in self.start_urls:\n",
        "        #     yield Request(url, dont_filter=True)\n",
        "\n",
        "    def parse(self, response):\n",
        "        items = SpiderAmazonItem()\n",
        "        title = response.xpath('//h1[@id=\"title\"]/span/text()').extract()\n",
        "        sale_price = response.xpath('//span[contains(@id,\"ourprice\") or contains(@id,\"saleprice\")]/text()').extract()\n",
        "        category = response.xpath('//a[@class=\"a-link-normal a-color-tertiary\"]/text()').extract()\n",
        "        availability = response.xpath('//div[@id=\"availability\"]//text()').extract()\n",
        "        items['product_name'] = ''.join(title).strip()\n",
        "        items['product_sale_price'] = ''.join(sale_price).strip()\n",
        "        items['product_category'] = ','.join(map(lambda x: x.strip(), category)).strip()\n",
        "        items['product_availability'] = ''.join(availability).strip()\n",
        "        yield items\n"
      ],
      "metadata": {
        "id": "ECnJMoNVfYIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот так уже можно запустить и получить необходимые результаты!"
      ],
      "metadata": {
        "id": "YVvCL2fcgwI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl AmazonProductSpider -o items.json #флаг -o - записать наш результат, запишем в виде jsonов (можно также в csv)"
      ],
      "metadata": {
        "id": "usYodh5zgznm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Чего не хватает?"
      ],
      "metadata": {
        "id": "Fus7p50DhEDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Понимать бы, как вообще работают url-ссылки (что они из себя представлют)\n",
        "\n",
        "* Хотим еще больше динамики для crawlingа - например, вводим запрос, получаем товары и ходим по ссылкам\n",
        "\n",
        "Это все реализуем уже на семинаре на примера другого маркетплейса)"
      ],
      "metadata": {
        "id": "Vi2KaXGehGSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Паук дня"
      ],
      "metadata": {
        "id": "bm4vHEvihZpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://live.staticflickr.com/4878/46966949422_871635f84f_b.jpg)"
      ],
      "metadata": {
        "id": "aPOph66ZhcBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это апулийский тарантул. Проживает в Италии, Испании и Португалии. Неядовитый (справедливости ради, подавляющее большинство пауков неядовитые, они могут укусить, но это бдет также, как и укус того же комара, неприятно, но нормально).\n",
        "\n",
        "Один из самых известных тарантулов. В Средневековье считалось, что укус этого паука вызывает безумие (и вообще, считали, что он почти все смертельные болезни переносил с собой, но это не так).\n",
        "\n",
        "Но что более интересно, что для того, чтобы не заболеть, итальянцы придумали противодействие: если укусили, то надо танцевать до упаду! А танец для того, чтобы не сойти с ума, назвали тарантелла (и он очень ассоциируется с Италией на данный момент)\n",
        "\n",
        "Вот насколько находящееся вокруг нас влияет на культуру)"
      ],
      "metadata": {
        "id": "SGOU_fI4BCu1"
      }
    }
  ]
}